\chapter{Total Decoupling}
In this chapter, we shall first describe the diagonalizing transformations
derived by \citet{Chu200896}, followed by the modifications needed for 
handling the case when zero is an eigenvalue. Then we shall have a brief 
discussion of the computational complexity involved and measures to decrease 
it. Before we start, let us a have brief overview of the symbols used,
which were previously introduced. We shall be using the \glspl{lam} \gls{a}
and \gls{b}, with the corresponding matrix pencil \gls{lp} and eigensystem
\gls{L}, \gls{X} and \gls{Y} $\in \mathbb{C}^{2n\times 2n}$ :
\begin{align}
	\gls{a} &\phantom{:}= \begin{bmatrix}
	-\gls{k} & \gls{0}\\
	\mathbf{0} & \gls{m}
	\end{bmatrix}\\
	\quad\gls{b} &\phantom{:}= \begin{bmatrix}
	\gls{c} & \gls{m}\\
	\gls{m} & \gls{0}
	\end{bmatrix}\\	
	\gls{lp} &:= \gls{b}\gls{l} - \gls{a} = 0
\end{align}
Except for \gls{j}, we shall not be using the matrices defined in \ref{sec:SPT}.
We are using normalized eigenvectors, so that:
\begin{subequations}
\label{eqn:norm_eigv}
\begin{align}
	\gls{Y}^T\gls{a}\gls{X} &= \gls{L}\\
	\gls{Y}^T\gls{b}\gls{X} &= \gls{I}_{2n}
\end{align}
\end{subequations}
Now let us scale the eigenvectors by their eigenvalues, thus:
\begin{align*}
	\label{eqn:vec_scale}
	\gls{X}^{[1]} &= \gls{X}\gls{L}^{-1/2}\\
	\gls{Y}^{[1]} &= \gls{Y}\gls{L}^{-1/2}
\end{align*}
Since \gls{L} is a diagonal matrix, its inverse is obtained by taking the 
reciprocals of its diagonal elements. The principle square root of the
diagonal elements is used. Then \eqref{eqn:norm_eigv} becomes
\begin{subequations}
\begin{align}
	\gls{Y}^{[1]T}\gls{a}\gls{X}^{[1]} &= \gls{I}_{2n}\\
	\gls{Y}^{[1]T}\gls{b}\gls{X}^{[1]} &= \gls{L}^{-1}
\end{align}
\end{subequations}
The remaining steps belong to three stages: classification of the eigenvalues
and their eigenvectors, elimination of complex values and scaling to restore
the transformed \gls{a} and \gls{b} to \glspl{lam}. The classification procedure 
and the derivation of the other matrices are provided in Appendix \ref{app:A}.
For now, we shall simply state the key matrices obtained:
\begin{subequations}
\label{eqn:pjfg}
\begin{align}
	\gls{p} &= \gls{I}_{2n}\left[\gls{ic}^+\gls{it}^+\gls{ic}^-\gls{it}^-\right]\\
%	
	\gls{j}_L &= \begin{bmatrix}
		\frac{1}{\sqrt{2}}\gls{I}_p & \gls{0} & \frac{-\iota}{\sqrt{2}}\gls{I}_p & \gls{0}\\
		\gls{0} & \mathbf{L}^+ & \gls{0} & \gls{0} \\
		\frac{1}{\sqrt{2}}\gls{I}_p & \gls{0} & \frac{\iota}{\sqrt{2}}\gls{I}_p & \gls{0}\\
		\gls{0} & \gls{0} & \gls{0} & \mathbf{L}^- \\
	\end{bmatrix}\\ 
	\gls{j}_R &= \begin{bmatrix}
		\frac{1}{\sqrt{2}}\gls{I}_p & \gls{0} & \frac{-\iota}{\sqrt{2}}\gls{I}_p & \gls{0}\\
		\gls{0} & \mathbf{R}^+ & \gls{0} & \gls{0} \\
		\frac{1}{\sqrt{2}}\gls{I}_p & \gls{0} & \frac{\iota}{\sqrt{2}}\gls{I}_p & \gls{0}\\
		\gls{0} & \gls{0} & \gls{0} & \mathbf{R}^- \\
	\end{bmatrix}\\
%
	\gls{F} &= \begin{bmatrix}
		\mathbf{\Phi} & \gls{0} & \gls{I}_{p} & \gls{0} \\
		\gls{0} & \mathbf{\Psi}^+ & \gls{0} & \gls{I}_{n-p} \\
		\gls{I}_{p} & \gls{0} & \mathbf{\Phi} & \gls{0} \\
		\gls{0} & \gls{I}_{n-p} & \gls{0} & \mathbf{\Psi}^-
	\end{bmatrix}\\
%
	\gls{G} &= \begin{bmatrix}
	\gls{I}_n & \gls{0} \\
	\gls{0} & \mathbf{\Theta}
	\end{bmatrix}
\end{align}
\end{subequations}
which give us the diagonalizing transformations \gls{Pi}[$_L$] and \gls{Pi}[$_R$]:
\begin{subequations}
\begin{align}
	\gls{Pi}_L = \gls{X}^{[1]}\gls{p}\gls{j}_L\gls{F}\gls{G}\\
	\gls{Pi}_R = \gls{X}^{[1]}\gls{p}\gls{j}_R\gls{F}\gls{G}
\end{align}
\end{subequations}
\gls{Pi}[$_L$] and \gls{Pi}[$_R$] give us the diagonalized system --- \gls{kd},
\gls{md} and \gls{cd}:
\begin{subequations}
\label{eqn:decoupled}
\begin{align}
	\gls{Pi}_L^T\gls{a}\gls{Pi}_R &= \begin{bmatrix}
	\gls{kd} & \gls{0} \\
	\gls{0} & \gls{md}\\
	\end{bmatrix}\\
	\gls{Pi}_L^T\gls{b}\gls{Pi}_R &= \begin{bmatrix}
	\gls{0} & \gls{md} \\
	\gls{md} & \gls{cd}\\
	\end{bmatrix}
\end{align}
\end{subequations}

\section{Modifications}
Consider the case when \gls{l}$_i = 0$, for some $ 1\leq i \leq 2n$. Then, 
the algorithm fails when at the very beginning, when scaling the eigenvectors 
using the eigenvalues \eqref{eqn:vec_scale}. The matrix form of the equation
immediately suggests an alternate route. Pseudoinverses of singular matrices
are very well known and well-defined. Therefore, we use the pseudoinverse of
the matrix of eigenvalues, specifically the Moore-Penrose pseudoinverse.
The inverse of a diagonal matrix is also a diagonal matrix with reciprocals 
of the corresponding diagonal terms. The pseudoinverse of a singular diagonal
matrix (obviously having atleast one zero in the diagonal), is also a diagonal
matrix with reciprocals of the corresponding diagonal terms, except for the 
zeros, which remain zero. Thus:
\begin{align*}
	{\begin{bmatrix}
		\gls{L}_1 & & \\		 
		 & \gls{0} & \\
		 & & \gls{L}_2	
	\end{bmatrix}}^{\gls{pinv}} = 
	\begin{bmatrix}
		\gls{L}_1^{-1} & & \\		 
		 & \gls{0} & \\
		 & & \gls{L}_2^{-1}	
	\end{bmatrix}
\end{align*}

Therefore, when we scale the eigenvectors using the pseudoinverse, the
columns of \gls{X} and \gls{Y} corresponding to the eigenvalue zero become
zero themselves. Whenever the inverse of a singular matrix is needed in 
the equations, we shall use the pseudoinverse instead. This may happen 
in the equations \eqref{eqn:phi}, \eqref{eqn:psi} and \eqref{eqn:theta}. 
The next problem arising when a \gls{l}$_i = 0$ is in the classification 
of eigenvalues (see Appendix \ref{list:class}). Here we may relax one each
of the three pairs of strict inequalities. The choice is quite arbitrary, since
the use pseudoinverses means that any cases division by 0 would be have 
a result equivalent to multiplication 0.
In any case, one must check for small non-zero values that maybe result of
round-off errors and truncation errors. These values should rightly be
exactly zero, but often aren't, and dividing by these small values
should be avoided.
 
Now let us consider the implications of using the pseudoinverse.
Since the the transformations represented by \gls{p}, \gls{j} and \gls{F}
are linear combinations of the scaled eigenvectors, the final \glspl{spe} 
obtained do not have full rank, and hence are not invertible. But, the 
filters obtained from these \glspl{spe} can be made invertible, using the 
concept of stable filters discussed in the next chapter. Consider the 
quadratic pencil \gls{qp} \eqref{eqn:quadratic_pencil}, now using the 
diagonalized matrices \gls{kd}, \gls{md} and \gls{cd}. Then we can write 
the pencil as separate polynomials in \gls{l}. If zero satisfies one of 
these polynomials, as it should, then one would expect that one of the
diagonal elements in \gls{kd} is zero. For each repeated occurrence of 
zero as an eigenvalue, a diagonal element in \gls{kd} would be zero. 

Unexpectedly, this is not what happens. Instead of ending up in \gls{kd}, 
the zeros end up in \gls{md}. Effectively, this means that, for each 
multiplicity of zero as an eigenvalue, one of the decoupled polynomial 
reduces its degree from quadratic to linear. The reduced polynomials
have a non-zero real eigenvalue as their root. Also, the corresponding
equation of motion now lacks an acceleration term! Studying the implications 
of this is one of the future goals of this project.

\section{Computational Improvements}
Like complex numbers, matrices are convenient for mathematics and rather 
inconvenient for computation. Multiplying two square matrices of size $n$
is an $O(n^3)$ task. Hence, while it is convenient to write 
$\gls{X}^{[1]} = \gls{X}\gls{p},$ for the purpose of computation,
selecting the individual columns is much faster. Also, the original equations
presented in \citet{Chu200896} use arbitrary eigenvectors, and scale them via 
the matrix multiplication $\gls{X}^{[1]} = \gls{X}\gls{b}_1^{-1/2}$, and
$\gls{Y}^{[1]} = \gls{Y}\gls{b}_1^{-1/2}$, where 
$\gls{b}_1 = \gls{Y}^T\gls{b}\gls{X}$. This involves 4 matrix 
multiplications, whereas solving for \gls{Y} in the equation 
\begin{align*}
	 \gls{I}_{2n} = \gls{Y}^T\gls{b}\gls{X}
\end{align*}
gives us $\gls{Y} = (\gls{X}\gls{b})^{-T}$, which saves us two matrix 
multiplications and the cost of calculating the left eigenvectors of
the system.

From similar arguments, the role of $\gls{j}_R,$ and $ \gls{j}_L$,
can be eliminated, reducing the matrix multiplications to obtaining 
multiples of eigenvectors or obtaining real or imaginary parts of the
eigenvectors, for example\footnote{Refer Appendix \ref{app:A} for an 
explanation of the subscripts.}:
\begin{subequations}
\label{eqn:exp_x}
\begin{align}
\gls{X}^{[2]} &= \gls{X}\gls{p}\gls{j}_R \\
&= \begin{bmatrix}
\sqrt{2}\gls{re}{\gls{X}_{\gls{ic}^+}} & \gls{X}_{\gls{it}^+}^{[1]} & \sqrt{2}\gls{im}{\gls{X}_{\gls{ic}^+}} & \gls{X}_{\gls{it}^+}^{[1]}
\end{bmatrix}\\
\gls{X}_{\gls{it}^+}^{[1]} &= \begin{bmatrix}
\gls{X}_{C_a^+} & \gls{X}_{C_b^+} & -\gls{im}{\gls{X}_{C_c^+}} & \gls{X}_{C_d^+} & -\gls{im}{\gls{X}_{C_e^+}} & \gls{X}_{C_f^+}
\end{bmatrix}\\
\gls{X}_{\gls{it}^+}^{[1]} &= \begin{bmatrix}
-\gls{im}{\gls{X}_{C_a^-}} & \gls{X}_{C_b^-} & -\gls{im}{\gls{X}_{C_c^-}} & \gls{X}_{C_d^-} & -\gls{im}{\gls{X}_{C_e^-}} & -\gls{im}{\gls{X}_{C_f^-}}
\end{bmatrix}
\end{align}
\end{subequations}
This can be extended further to \gls{F} and \gls{G}. Expanding these matrix 
products can, and does, give us the values of the diagonalized matrices
directly. An algorithm using simplified results obtained from expanding the 
matrix products is provided in \ref{app:2}. As one may observe, complex 
arithmetic is not used. In fact, the algorithm reveals that 
complex calculations are not necessary at all for derivation of the \glspl{spe},
and one may simply store the real and imaginary parts of the complex values 
involved individually without being inconvenienced about complex calculations.

\section{An Example}
For an example of decoupling from this procedure, we shall make use of
a system with an eigenvalue equal to zero, since both \citet{Chu200896}
and \citet{GARVEY2002885} present examples where no eigenvalues are zero.
The system matrices are:
\begin{align*}
\gls{m} &= \begin{bmatrix}
 0.7621  &     0.4447   &    0.7382    &   0.9169\\
   0.4565  &     0.6154    &   0.1763    &   0.4103\\
   0.0185   &    0.7919   &    0.4057    &   0.8936\\
   0.8214   &    0.9218   &    0.9355     &  0.0579
\end{bmatrix}\\
\gls{k} &= \begin{bmatrix}
0.2111   &   -0.6014   &  -0.48997   &    1.2366\\
   1.1902  &     0.5512  &    0.44908   &   -0.6313\\
  -1.1162   &   -1.0998  &   -0.89603   &   -2.3252\\
   0.6353   &     0.086  &   0.070066   &   -1.2316
\end{bmatrix}\\
\gls{c} &= \begin{bmatrix}
 0.371  &    -1.0226  &     0.3155   &    0.5045\\
   0.7283   &    1.0378   &    1.5532  &     1.8645\\
   2.1122   &   -0.3898   &    0.7079   &   -0.3398\\
  -1.3573  &    -1.3813   &    1.9574   &   -1.1398
\end{bmatrix}
\end{align*}
Here, an eigenvalue is zero because \gls{k} is singular, and its nullity is 1.
The eigenvalues (sorted by absolute value) are:
\begin{align*}
\left\lbrace\begin{matrix}
 0, & -0.13669 \pm 0.47519\gls{i}, & -1.7264 \pm 0.36492\gls{i}, & 0.087312 \pm 2.4286\gls{i}, & 3.4173
\end{matrix}\right\rbrace\\
\end{align*}
The \glspl{spe} are:
\begin{align*}
\gls{Pi}_L &= \begin{bmatrix}
 -4.3438 & -9.3035 &  0.9329 &  3.8414 & 0.64854 &  2.3448 &  1.8965 & -1.3706\\
 -4.0555 & -3.2867 & 0.38837 &  2.0024 &   5.956 &  4.4991 & -0.61423 & -2.7666\\
 -38.405 &  67.603 &  7.7113 & -2.1625 & -14.964 &  21.717 &  -70.529 & -49.728\\
 -0.071343 & -0.43711 & 0.77466 & -0.043924 & -0.2438 & -1.4938 & 2.6473 & -0.1501\\
 -2.6527 & -9.5906 & -7.7571 &   5.606 & -3.6186 & -6.6816 &   3.0535 &  2.3088\\
 -1.9128 & -1.4449 & 0.19726 & 0.88851 &  2.5491 &  1.7025 &   -0.29276 & -1.0655\\
  2.5338 & -3.6772 &  11.942 &    8.42 & -37.963 &  66.961  &  9.7967  &   -0.69221\\
  0   & 0 &   0 &   0   & 0   & 0 &   0 & 0
\end{bmatrix}\\
\gls{Pi}_R &= \begin{bmatrix}
7.4028 & -0.08227 & -0.32704 &  -0.1583 &  -10.226 & -0.059461 &   1.1118 & 0 \\
-7.626 & 0.074557 &  -14.944 & 0.015977 &    15.84 & 0.080423 &   1.4468 &  0 \\
8.9447 &  0.32858 &   6.8018 &  0.06258 &   16.439 &  0.28605 &   3.5329 &  0 \\
-0.68185 & -0.28094 &   4.5378 & 0.071865 &   2.6872 & -0.21892 &  -1.7018 & 0 \\
2.5002 &  0.18514 &  -6.5661 & -0.54095 &   10.198 &  0.12304 &  -0.1329 &  0 \\
-3.8727 & -0.25042 &  -8.5445 & 0.054599 &  -11.956 & -0.20313 &  -14.691 & 0 \\
-4.0191 & -0.89067 &  -20.865 &  0.21386 &   4.4507 & -0.65909 &   7.4188 & 0 \\
-0.65698 &  0.68167 &   10.051 &  0.24559 &  -1.4164 &  0.47497 &   4.2406 & 0
\end{bmatrix}
\end{align*}
Obviously, both \glspl{spe} are singular, but a column or row of zeros need not
be present for all such cases. And the diagonalized matrices:
\begin{align*}
\gls{md} &= diag\begin{bmatrix}
 -201.74  &   -0.16746   &   -524.19   &         0
\end{bmatrix}\\
\gls{kd} &= diag\begin{bmatrix}
      -49.323  &   -0.52144   &   -3095.8     &      -1
\end{bmatrix}\\
\gls{cd} &= diag\begin{bmatrix}
      -55.151   &  -0.57823  &     91.537  &    0.29263
\end{bmatrix}
\end{align*}
As we can see, a diagonal element of \gls{md} is 0, and the 
corresponding matrix pencil \gls{lp}$ := 0.29263\gls{l} - 1 = 0$ has 
the solution $\gls{l} =3.4173,$ the only other real eigenvalue. 


















